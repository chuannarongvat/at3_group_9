{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a0ace32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extracted CSV files: 676\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Specify the path to the folder containing ZIP files\n",
    "folder_path = r\"C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\itineraries_csv\\itineraries_csv\\ATL\"\n",
    "\n",
    "# Specify the directory where you want to extract the CSV files\n",
    "output_dir = \"extracted_csv\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# List all ZIP files in the specified folder\n",
    "zip_files = [file for file in os.listdir(folder_path) if file.endswith(\".zip\")]\n",
    "\n",
    "# Extract CSV files from each ZIP file directly to the output folder\n",
    "for zip_file in zip_files:\n",
    "    zip_file_path = os.path.join(folder_path, zip_file)\n",
    "    \n",
    "    # Extract the ZIP file to the output directory\n",
    "    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "\n",
    "# Count the number of extracted CSV files\n",
    "num_extracted_csv_files = len([file for file in os.listdir(output_dir) if file.endswith(\".csv\")])\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of extracted CSV files: {num_extracted_csv_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bea83e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extracted CSV files: 1352\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Specify the path to the folder containing ZIP files\n",
    "folder_path = r\"C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\itineraries_csv\\itineraries_csv\\BOS\"\n",
    "\n",
    "# Specify the directory where you want to extract the CSV files\n",
    "output_dir = \"extracted_csv\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# List all ZIP files in the specified folder\n",
    "zip_files = [file for file in os.listdir(folder_path) if file.endswith(\".zip\")]\n",
    "\n",
    "# Extract CSV files from each ZIP file directly to the output folder\n",
    "for zip_file in zip_files:\n",
    "    zip_file_path = os.path.join(folder_path, zip_file)\n",
    "    \n",
    "    # Extract the ZIP file to the output directory\n",
    "    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "\n",
    "# Count the number of extracted CSV files\n",
    "num_extracted_csv_files = len([file for file in os.listdir(output_dir) if file.endswith(\".csv\")])\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of extracted CSV files: {num_extracted_csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f47f779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extracted CSV files: 2028\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Specify the path to the folder containing ZIP files\n",
    "folder_path = r\"C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\itineraries_csv\\itineraries_csv\\CLT\"\n",
    "\n",
    "# Specify the directory where you want to extract the CSV files\n",
    "output_dir = \"extracted_csv\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# List all ZIP files in the specified folder\n",
    "zip_files = [file for file in os.listdir(folder_path) if file.endswith(\".zip\")]\n",
    "\n",
    "# Extract CSV files from each ZIP file directly to the output folder\n",
    "for zip_file in zip_files:\n",
    "    zip_file_path = os.path.join(folder_path, zip_file)\n",
    "    \n",
    "    # Extract the ZIP file to the output directory\n",
    "    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "\n",
    "# Count the number of extracted CSV files\n",
    "num_extracted_csv_files = len([file for file in os.listdir(output_dir) if file.endswith(\".csv\")])\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of extracted CSV files: {num_extracted_csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d7d46e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extracted CSV files: 2704\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Specify the path to the folder containing ZIP files\n",
    "folder_path = r\"C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\itineraries_csv\\itineraries_csv\\DEN\"\n",
    "\n",
    "# Specify the directory where you want to extract the CSV files\n",
    "output_dir = \"extracted_csv\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# List all ZIP files in the specified folder\n",
    "zip_files = [file for file in os.listdir(folder_path) if file.endswith(\".zip\")]\n",
    "\n",
    "# Extract CSV files from each ZIP file directly to the output folder\n",
    "for zip_file in zip_files:\n",
    "    zip_file_path = os.path.join(folder_path, zip_file)\n",
    "    \n",
    "    # Extract the ZIP file to the output directory\n",
    "    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "\n",
    "# Count the number of extracted CSV files\n",
    "num_extracted_csv_files = len([file for file in os.listdir(output_dir) if file.endswith(\".csv\")])\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of extracted CSV files: {num_extracted_csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60cdfc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extracted CSV files: 3380\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Specify the path to the folder containing ZIP files\n",
    "folder_path = r\"C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\itineraries_csv\\itineraries_csv\\DFW\"\n",
    "\n",
    "# Specify the directory where you want to extract the CSV files\n",
    "output_dir = \"extracted_csv\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# List all ZIP files in the specified folder\n",
    "zip_files = [file for file in os.listdir(folder_path) if file.endswith(\".zip\")]\n",
    "\n",
    "# Extract CSV files from each ZIP file directly to the output folder\n",
    "for zip_file in zip_files:\n",
    "    zip_file_path = os.path.join(folder_path, zip_file)\n",
    "    \n",
    "    # Extract the ZIP file to the output directory\n",
    "    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "\n",
    "# Count the number of extracted CSV files\n",
    "num_extracted_csv_files = len([file for file in os.listdir(output_dir) if file.endswith(\".csv\")])\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of extracted CSV files: {num_extracted_csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcde1014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extracted CSV files: 4056\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Specify the path to the folder containing ZIP files\n",
    "folder_path = r\"C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\itineraries_csv\\itineraries_csv\\DTW\"\n",
    "\n",
    "# Specify the directory where you want to extract the CSV files\n",
    "output_dir = \"extracted_csv\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# List all ZIP files in the specified folder\n",
    "zip_files = [file for file in os.listdir(folder_path) if file.endswith(\".zip\")]\n",
    "\n",
    "# Extract CSV files from each ZIP file directly to the output folder\n",
    "for zip_file in zip_files:\n",
    "    zip_file_path = os.path.join(folder_path, zip_file)\n",
    "    \n",
    "    # Extract the ZIP file to the output directory\n",
    "    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "\n",
    "# Count the number of extracted CSV files\n",
    "num_extracted_csv_files = len([file for file in os.listdir(output_dir) if file.endswith(\".csv\")])\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of extracted CSV files: {num_extracted_csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3e2f97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extracted CSV files: 4732\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Specify the path to the folder containing ZIP files\n",
    "folder_path = r\"C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\itineraries_csv\\itineraries_csv\\EWR\"\n",
    "\n",
    "# Specify the directory where you want to extract the CSV files\n",
    "output_dir = \"extracted_csv\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# List all ZIP files in the specified folder\n",
    "zip_files = [file for file in os.listdir(folder_path) if file.endswith(\".zip\")]\n",
    "\n",
    "# Extract CSV files from each ZIP file directly to the output folder\n",
    "for zip_file in zip_files:\n",
    "    zip_file_path = os.path.join(folder_path, zip_file)\n",
    "    \n",
    "    # Extract the ZIP file to the output directory\n",
    "    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "\n",
    "# Count the number of extracted CSV files\n",
    "num_extracted_csv_files = len([file for file in os.listdir(output_dir) if file.endswith(\".csv\")])\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of extracted CSV files: {num_extracted_csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3b991d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extracted CSV files: 5408\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Specify the path to the folder containing ZIP files\n",
    "folder_path = r\"C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\itineraries_csv\\itineraries_csv\\IAD\"\n",
    "\n",
    "# Specify the directory where you want to extract the CSV files\n",
    "output_dir = \"extracted_csv\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# List all ZIP files in the specified folder\n",
    "zip_files = [file for file in os.listdir(folder_path) if file.endswith(\".zip\")]\n",
    "\n",
    "# Extract CSV files from each ZIP file directly to the output folder\n",
    "for zip_file in zip_files:\n",
    "    zip_file_path = os.path.join(folder_path, zip_file)\n",
    "    \n",
    "    # Extract the ZIP file to the output directory\n",
    "    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "\n",
    "# Count the number of extracted CSV files\n",
    "num_extracted_csv_files = len([file for file in os.listdir(output_dir) if file.endswith(\".csv\")])\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of extracted CSV files: {num_extracted_csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "165bdc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extracted CSV files: 6084\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Specify the path to the folder containing ZIP files\n",
    "folder_path = r\"C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\itineraries_csv\\itineraries_csv\\JFK\"\n",
    "\n",
    "# Specify the directory where you want to extract the CSV files\n",
    "output_dir = \"extracted_csv\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# List all ZIP files in the specified folder\n",
    "zip_files = [file for file in os.listdir(folder_path) if file.endswith(\".zip\")]\n",
    "\n",
    "# Extract CSV files from each ZIP file directly to the output folder\n",
    "for zip_file in zip_files:\n",
    "    zip_file_path = os.path.join(folder_path, zip_file)\n",
    "    \n",
    "    # Extract the ZIP file to the output directory\n",
    "    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "\n",
    "# Count the number of extracted CSV files\n",
    "num_extracted_csv_files = len([file for file in os.listdir(output_dir) if file.endswith(\".csv\")])\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of extracted CSV files: {num_extracted_csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1e6f553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extracted CSV files: 6760\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Specify the path to the folder containing ZIP files\n",
    "folder_path = r\"C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\itineraries_csv\\itineraries_csv\\LAX\"\n",
    "\n",
    "# Specify the directory where you want to extract the CSV files\n",
    "output_dir = \"extracted_csv\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# List all ZIP files in the specified folder\n",
    "zip_files = [file for file in os.listdir(folder_path) if file.endswith(\".zip\")]\n",
    "\n",
    "# Extract CSV files from each ZIP file directly to the output folder\n",
    "for zip_file in zip_files:\n",
    "    zip_file_path = os.path.join(folder_path, zip_file)\n",
    "    \n",
    "    # Extract the ZIP file to the output directory\n",
    "    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "\n",
    "# Count the number of extracted CSV files\n",
    "num_extracted_csv_files = len([file for file in os.listdir(output_dir) if file.endswith(\".csv\")])\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of extracted CSV files: {num_extracted_csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7986b05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extracted CSV files: 7436\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Specify the path to the folder containing ZIP files\n",
    "folder_path = r\"C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\itineraries_csv\\itineraries_csv\\LGA\"\n",
    "\n",
    "# Specify the directory where you want to extract the CSV files\n",
    "output_dir = \"extracted_csv\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# List all ZIP files in the specified folder\n",
    "zip_files = [file for file in os.listdir(folder_path) if file.endswith(\".zip\")]\n",
    "\n",
    "# Extract CSV files from each ZIP file directly to the output folder\n",
    "for zip_file in zip_files:\n",
    "    zip_file_path = os.path.join(folder_path, zip_file)\n",
    "    \n",
    "    # Extract the ZIP file to the output directory\n",
    "    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "\n",
    "# Count the number of extracted CSV files\n",
    "num_extracted_csv_files = len([file for file in os.listdir(output_dir) if file.endswith(\".csv\")])\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of extracted CSV files: {num_extracted_csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b7968b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extracted CSV files: 8112\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Specify the path to the folder containing ZIP files\n",
    "folder_path = r\"C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\itineraries_csv\\itineraries_csv\\MIA\"\n",
    "\n",
    "# Specify the directory where you want to extract the CSV files\n",
    "output_dir = \"extracted_csv\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# List all ZIP files in the specified folder\n",
    "zip_files = [file for file in os.listdir(folder_path) if file.endswith(\".zip\")]\n",
    "\n",
    "# Extract CSV files from each ZIP file directly to the output folder\n",
    "for zip_file in zip_files:\n",
    "    zip_file_path = os.path.join(folder_path, zip_file)\n",
    "    \n",
    "    # Extract the ZIP file to the output directory\n",
    "    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "\n",
    "# Count the number of extracted CSV files\n",
    "num_extracted_csv_files = len([file for file in os.listdir(output_dir) if file.endswith(\".csv\")])\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of extracted CSV files: {num_extracted_csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b3eb1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extracted CSV files: 8788\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Specify the path to the folder containing ZIP files\n",
    "folder_path = r\"C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\itineraries_csv\\itineraries_csv\\OAK\"\n",
    "\n",
    "# Specify the directory where you want to extract the CSV files\n",
    "output_dir = \"extracted_csv\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# List all ZIP files in the specified folder\n",
    "zip_files = [file for file in os.listdir(folder_path) if file.endswith(\".zip\")]\n",
    "\n",
    "# Extract CSV files from each ZIP file directly to the output folder\n",
    "for zip_file in zip_files:\n",
    "    zip_file_path = os.path.join(folder_path, zip_file)\n",
    "    \n",
    "    # Extract the ZIP file to the output directory\n",
    "    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "\n",
    "# Count the number of extracted CSV files\n",
    "num_extracted_csv_files = len([file for file in os.listdir(output_dir) if file.endswith(\".csv\")])\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of extracted CSV files: {num_extracted_csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c899f065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extracted CSV files: 9464\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Specify the path to the folder containing ZIP files\n",
    "folder_path = r\"C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\itineraries_csv\\itineraries_csv\\ORD\"\n",
    "\n",
    "# Specify the directory where you want to extract the CSV files\n",
    "output_dir = \"extracted_csv\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# List all ZIP files in the specified folder\n",
    "zip_files = [file for file in os.listdir(folder_path) if file.endswith(\".zip\")]\n",
    "\n",
    "# Extract CSV files from each ZIP file directly to the output folder\n",
    "for zip_file in zip_files:\n",
    "    zip_file_path = os.path.join(folder_path, zip_file)\n",
    "    \n",
    "    # Extract the ZIP file to the output directory\n",
    "    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "\n",
    "# Count the number of extracted CSV files\n",
    "num_extracted_csv_files = len([file for file in os.listdir(output_dir) if file.endswith(\".csv\")])\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of extracted CSV files: {num_extracted_csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11138cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extracted CSV files: 10140\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Specify the path to the folder containing ZIP files\n",
    "folder_path = r\"C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\itineraries_csv\\itineraries_csv\\PHL\"\n",
    "\n",
    "# Specify the directory where you want to extract the CSV files\n",
    "output_dir = \"extracted_csv\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# List all ZIP files in the specified folder\n",
    "zip_files = [file for file in os.listdir(folder_path) if file.endswith(\".zip\")]\n",
    "\n",
    "# Extract CSV files from each ZIP file directly to the output folder\n",
    "for zip_file in zip_files:\n",
    "    zip_file_path = os.path.join(folder_path, zip_file)\n",
    "    \n",
    "    # Extract the ZIP file to the output directory\n",
    "    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "\n",
    "# Count the number of extracted CSV files\n",
    "num_extracted_csv_files = len([file for file in os.listdir(output_dir) if file.endswith(\".csv\")])\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of extracted CSV files: {num_extracted_csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26849973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extracted CSV files: 10816\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Specify the path to the folder containing ZIP files\n",
    "folder_path = r\"C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\itineraries_csv\\itineraries_csv\\SFO\"\n",
    "\n",
    "# Specify the directory where you want to extract the CSV files\n",
    "output_dir = \"extracted_csv\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# List all ZIP files in the specified folder\n",
    "zip_files = [file for file in os.listdir(folder_path) if file.endswith(\".zip\")]\n",
    "\n",
    "# Extract CSV files from each ZIP file directly to the output folder\n",
    "for zip_file in zip_files:\n",
    "    zip_file_path = os.path.join(folder_path, zip_file)\n",
    "    \n",
    "    # Extract the ZIP file to the output directory\n",
    "    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "\n",
    "# Count the number of extracted CSV files\n",
    "num_extracted_csv_files = len([file for file in os.listdir(output_dir) if file.endswith(\".csv\")])\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of extracted CSV files: {num_extracted_csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b1fc201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: Converted 1000 files to FTR\n",
      "Batch 2: Converted 1000 files to FTR\n",
      "Batch 3: Converted 1000 files to FTR\n",
      "Batch 4: Converted 1000 files to FTR\n",
      "Batch 5: Converted 1000 files to FTR\n",
      "Batch 6: Converted 1000 files to FTR\n",
      "Batch 7: Converted 1000 files to FTR\n",
      "Batch 8: Converted 1000 files to FTR\n",
      "Batch 9: Converted 1000 files to FTR\n",
      "Batch 10: Converted 1000 files to FTR\n",
      "Batch 11: Converted 816 files to FTR\n",
      "Total number of CSV files converted to FTR: 10816\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.feather as feather\n",
    "\n",
    "# Specify the directory where the CSV files are located\n",
    "csv_folder = r\"C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\extracted_csv\"\n",
    "\n",
    "# Specify the directory where you want to save the FTR (Feather) files\n",
    "ftr_folder = r\"C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\extracted_ftr\"\n",
    "\n",
    "# Create the FTR folder if it doesn't exist\n",
    "if not os.path.exists(ftr_folder):\n",
    "    os.makedirs(ftr_folder)\n",
    "\n",
    "# List all CSV files in the specified folder\n",
    "csv_files = [file for file in os.listdir(csv_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "# Set the batch size for conversion (e.g., 1000 files per round)\n",
    "batch_size = 1000\n",
    "\n",
    "# Loop through the CSV files in batches, convert to FTR, and save them\n",
    "num_converted_files = 0\n",
    "\n",
    "for i in range(0, len(csv_files), batch_size):\n",
    "    batch_files = csv_files[i:i + batch_size]\n",
    "    \n",
    "    for csv_file in batch_files:\n",
    "        # Read the CSV file\n",
    "        csv_file_path = os.path.join(csv_folder, csv_file)\n",
    "        data = pd.read_csv(csv_file_path)\n",
    "\n",
    "        # Define the FTR file name\n",
    "        ftr_file = os.path.splitext(csv_file)[0] + \".ftr\"\n",
    "        ftr_file_path = os.path.join(ftr_folder, ftr_file)\n",
    "\n",
    "        # Convert and save as FTR\n",
    "        table = pa.Table.from_pandas(data)\n",
    "        feather.write_feather(table, ftr_file_path)\n",
    "        \n",
    "        num_converted_files += 1\n",
    "\n",
    "    # Print the number of files converted in this round\n",
    "    print(f\"Batch {i // batch_size + 1}: Converted {len(batch_files)} files to FTR\")\n",
    "\n",
    "# Print the total number of converted FTR files\n",
    "print(f\"Total number of CSV files converted to FTR: {num_converted_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73ed7728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: Converted 1000 files to Parquet\n",
      "Batch 2: Converted 1000 files to Parquet\n",
      "Batch 3: Converted 1000 files to Parquet\n",
      "Batch 4: Converted 1000 files to Parquet\n",
      "Batch 5: Converted 1000 files to Parquet\n",
      "Batch 6: Converted 1000 files to Parquet\n",
      "Batch 7: Converted 1000 files to Parquet\n",
      "Batch 8: Converted 1000 files to Parquet\n",
      "Batch 9: Converted 1000 files to Parquet\n",
      "Batch 10: Converted 1000 files to Parquet\n",
      "Batch 11: Converted 816 files to Parquet\n",
      "Total number of CSV files converted to Parquet: 10816\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Specify the directory where the CSV files are located\n",
    "csv_folder = r\"C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\extracted_csv\"\n",
    "\n",
    "# Specify the directory where you want to save the Parquet files\n",
    "parquet_folder = r\"C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\extracted_parquet\"\n",
    "\n",
    "# Create the Parquet folder if it doesn't exist\n",
    "if not os.path.exists(parquet_folder):\n",
    "    os.makedirs(parquet_folder)\n",
    "\n",
    "# List all CSV files in the specified folder\n",
    "csv_files = [file for file in os.listdir(csv_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "# Set the batch size for conversion (e.g., 1000 files per round)\n",
    "batch_size = 1000\n",
    "\n",
    "# Loop through the CSV files in batches, convert to Parquet, and save them\n",
    "num_converted_files = 0\n",
    "\n",
    "for i in range(0, len(csv_files), batch_size):\n",
    "    batch_files = csv_files[i:i + batch_size]\n",
    "    \n",
    "    for csv_file in batch_files:\n",
    "        # Read the CSV file\n",
    "        csv_file_path = os.path.join(csv_folder, csv_file)\n",
    "        data = pd.read_csv(csv_file_path)\n",
    "\n",
    "        # Define the Parquet file name\n",
    "        parquet_file = os.path.splitext(csv_file)[0] + \".parquet\"\n",
    "        parquet_file_path = os.path.join(parquet_folder, parquet_file)\n",
    "\n",
    "        # Convert and save as Parquet\n",
    "        table = pa.Table.from_pandas(data)\n",
    "        with open(parquet_file_path, 'wb') as f:\n",
    "            pq.write_table(table, f)\n",
    "\n",
    "        num_converted_files += 1\n",
    "\n",
    "    # Print the number of files converted in this round\n",
    "    print(f\"Batch {i // batch_size + 1}: Converted {len(batch_files)} files to Parquet\")\n",
    "\n",
    "# Print the total number of converted Parquet files\n",
    "print(f\"Total number of CSV files converted to Parquet: {num_converted_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a17fde03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\sudar\\anaconda3\\lib\\site-packages (11.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\sudar\\anaconda3\\lib\\site-packages (from pyarrow) (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2d8a366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged batch 0 into merged_batch_0.parquet\n",
      "Merged batch 1 into merged_batch_1.parquet\n",
      "Merged batch 2 into merged_batch_2.parquet\n",
      "Merged batch 3 into merged_batch_3.parquet\n",
      "Merged batch 4 into merged_batch_4.parquet\n",
      "Merged batch 5 into merged_batch_5.parquet\n",
      "Merged batch 6 into merged_batch_6.parquet\n",
      "Merged batch 7 into merged_batch_7.parquet\n",
      "Merged batch 8 into merged_batch_8.parquet\n",
      "Merged batch 9 into merged_batch_9.parquet\n",
      "Merged the last batch into merged_batch_10.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "# Set the directory where your Parquet files are located\n",
    "directory = r'C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\extracted_parquet'\n",
    "\n",
    "# Get a list of all Parquet files in the directory\n",
    "parquet_files = [file for file in os.listdir(directory) if file.endswith('.parquet')]\n",
    "\n",
    "# Define the batch size (number of files to merge in each round)\n",
    "batch_size = 1000\n",
    "\n",
    "# Initialize variables for tracking the current batch\n",
    "current_batch = []\n",
    "batch_count = 0\n",
    "\n",
    "# Loop through the Parquet files and merge them in batches\n",
    "for file in parquet_files:\n",
    "    current_batch.append(os.path.join(directory, file))\n",
    "    \n",
    "    # Check if the batch size is reached\n",
    "    if len(current_batch) == batch_size:\n",
    "        # Merge the current batch of files into a single Parquet file\n",
    "        merged_file = f'merged_batch_{batch_count}.parquet'\n",
    "        tables_to_concatenate = [pq.read_table(file) for file in current_batch]\n",
    "        concatenated_table = pa.concat_tables(tables_to_concatenate)\n",
    "        pq.write_table(concatenated_table, merged_file)\n",
    "        print(f'Merged batch {batch_count} into {merged_file}')\n",
    "        \n",
    "        # Clear the current batch and increment the batch count\n",
    "        current_batch = []\n",
    "        batch_count += 1\n",
    "\n",
    "# Merge any remaining files in the last batch\n",
    "if current_batch:\n",
    "    merged_file = f'merged_batch_{batch_count}.parquet'\n",
    "    tables_to_concatenate = [pq.read_table(file) for file in current_batch]\n",
    "    concatenated_table = pa.concat_tables(tables_to_concatenate)\n",
    "    pq.write_table(concatenated_table, merged_file)\n",
    "    print(f'Merged the last batch into {merged_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6fec8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Parquet files merged into C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\merged_all.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "# Set the directory where your Parquet files are located\n",
    "directory = r'C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\extracted_parquet'\n",
    "\n",
    "# Get a list of all Parquet files in the directory\n",
    "parquet_files = [file for file in os.listdir(directory) if file.endswith('.parquet')]\n",
    "\n",
    "# Initialize a list to hold the tables from all files\n",
    "tables_to_concatenate = []\n",
    "\n",
    "# Loop through the Parquet files and read them into tables\n",
    "for file in parquet_files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    table = pq.read_table(file_path)\n",
    "    tables_to_concatenate.append(table)\n",
    "\n",
    "# Concatenate all the tables into one table\n",
    "concatenated_table = pa.concat_tables(tables_to_concatenate)\n",
    "\n",
    "# Save the concatenated table as a single Parquet file\n",
    "output_file = r'C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\merged_all.parquet'\n",
    "pq.write_table(concatenated_table, output_file)\n",
    "\n",
    "print(f'All Parquet files merged into {output_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4dbedd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2d4c1eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 13519999\n",
      "Number of columns: 23\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Path to the merged Parquet file\n",
    "parquet_file = r'C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\merged_all.parquet'\n",
    "#parquet_file = r'C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\merged_batch_5.parquet'\n",
    "\n",
    "# Read the Parquet file into a PyArrow table\n",
    "table = pq.read_table(parquet_file)\n",
    "\n",
    "# Convert the PyArrow table to a Pandas DataFrame\n",
    "df = table.to_pandas()\n",
    "\n",
    "num_rows, num_cols = df.shape\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_cols}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d937972b",
   "metadata": {},
   "source": [
    "Number of rows: 13,519,999\n",
    "Number of columns: 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a6363fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>legId</th>\n",
       "      <th>searchDate</th>\n",
       "      <th>flightDate</th>\n",
       "      <th>startingAirport</th>\n",
       "      <th>destinationAirport</th>\n",
       "      <th>travelDuration</th>\n",
       "      <th>isBasicEconomy</th>\n",
       "      <th>isRefundable</th>\n",
       "      <th>isNonStop</th>\n",
       "      <th>totalFare</th>\n",
       "      <th>...</th>\n",
       "      <th>segmentsArrivalTimeEpochSeconds</th>\n",
       "      <th>segmentsArrivalTimeRaw</th>\n",
       "      <th>segmentsArrivalAirportCode</th>\n",
       "      <th>segmentsDepartureAirportCode</th>\n",
       "      <th>segmentsAirlineName</th>\n",
       "      <th>segmentsAirlineCode</th>\n",
       "      <th>segmentsEquipmentDescription</th>\n",
       "      <th>segmentsDurationInSeconds</th>\n",
       "      <th>segmentsDistance</th>\n",
       "      <th>segmentsCabinCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9ca0e81111c683bec1012473feefd28f</td>\n",
       "      <td>2022-04-16</td>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>ATL</td>\n",
       "      <td>BOS</td>\n",
       "      <td>PT2H29M</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>248.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1650223560</td>\n",
       "      <td>2022-04-17T15:26:00.000-04:00</td>\n",
       "      <td>BOS</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Delta</td>\n",
       "      <td>DL</td>\n",
       "      <td>Airbus A321</td>\n",
       "      <td>8940</td>\n",
       "      <td>947</td>\n",
       "      <td>coach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98685953630e772a098941b71906592b</td>\n",
       "      <td>2022-04-16</td>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>ATL</td>\n",
       "      <td>BOS</td>\n",
       "      <td>PT2H30M</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>248.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1650200400</td>\n",
       "      <td>2022-04-17T09:00:00.000-04:00</td>\n",
       "      <td>BOS</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Delta</td>\n",
       "      <td>DL</td>\n",
       "      <td>Airbus A321</td>\n",
       "      <td>9000</td>\n",
       "      <td>947</td>\n",
       "      <td>coach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98d90cbc32bfbb05c2fc32897c7c1087</td>\n",
       "      <td>2022-04-16</td>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>ATL</td>\n",
       "      <td>BOS</td>\n",
       "      <td>PT2H30M</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>248.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1650218700</td>\n",
       "      <td>2022-04-17T14:05:00.000-04:00</td>\n",
       "      <td>BOS</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Delta</td>\n",
       "      <td>DL</td>\n",
       "      <td>Boeing 757-200</td>\n",
       "      <td>9000</td>\n",
       "      <td>947</td>\n",
       "      <td>coach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>969a269d38eae583f455486fa90877b4</td>\n",
       "      <td>2022-04-16</td>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>ATL</td>\n",
       "      <td>BOS</td>\n",
       "      <td>PT2H32M</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>248.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1650227460</td>\n",
       "      <td>2022-04-17T16:31:00.000-04:00</td>\n",
       "      <td>BOS</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Delta</td>\n",
       "      <td>DL</td>\n",
       "      <td>Airbus A321</td>\n",
       "      <td>9120</td>\n",
       "      <td>947</td>\n",
       "      <td>coach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>980370cf27c89b40d2833a1d5afc9751</td>\n",
       "      <td>2022-04-16</td>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>ATL</td>\n",
       "      <td>BOS</td>\n",
       "      <td>PT2H34M</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>248.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1650213180</td>\n",
       "      <td>2022-04-17T12:33:00.000-04:00</td>\n",
       "      <td>BOS</td>\n",
       "      <td>ATL</td>\n",
       "      <td>Delta</td>\n",
       "      <td>DL</td>\n",
       "      <td>Airbus A321</td>\n",
       "      <td>9240</td>\n",
       "      <td>947</td>\n",
       "      <td>coach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>79eda9f841e226a1e2121d74211e595c</td>\n",
       "      <td>2022-04-16</td>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>ATL</td>\n",
       "      <td>BOS</td>\n",
       "      <td>PT2H38M</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>248.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1650216180</td>\n",
       "      <td>2022-04-17T13:23:00.000-04:00</td>\n",
       "      <td>BOS</td>\n",
       "      <td>ATL</td>\n",
       "      <td>JetBlue Airways</td>\n",
       "      <td>B6</td>\n",
       "      <td>None</td>\n",
       "      <td>9480</td>\n",
       "      <td>947</td>\n",
       "      <td>coach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9335fae376c38bb61263281779f469ec</td>\n",
       "      <td>2022-04-16</td>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>ATL</td>\n",
       "      <td>BOS</td>\n",
       "      <td>PT4H12M</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>251.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1650203400||1650213120</td>\n",
       "      <td>2022-04-17T09:50:00.000-04:00||2022-04-17T12:3...</td>\n",
       "      <td>CLT||BOS</td>\n",
       "      <td>ATL||CLT</td>\n",
       "      <td>American Airlines||American Airlines</td>\n",
       "      <td>AA||AA</td>\n",
       "      <td>Airbus A320||Airbus A320</td>\n",
       "      <td>5400||7500</td>\n",
       "      <td>228||728</td>\n",
       "      <td>coach||coach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3904bf87f2d1daf334f1ae7e3b876028</td>\n",
       "      <td>2022-04-16</td>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>ATL</td>\n",
       "      <td>BOS</td>\n",
       "      <td>PT5H18M</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>251.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1650203400||1650217080</td>\n",
       "      <td>2022-04-17T09:50:00.000-04:00||2022-04-17T13:3...</td>\n",
       "      <td>CLT||BOS</td>\n",
       "      <td>ATL||CLT</td>\n",
       "      <td>American Airlines||American Airlines</td>\n",
       "      <td>AA||AA</td>\n",
       "      <td>Airbus A320||Boeing 737-800</td>\n",
       "      <td>5400||8280</td>\n",
       "      <td>228||728</td>\n",
       "      <td>coach||coach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>d93988734c44a3c075d9efe373352507</td>\n",
       "      <td>2022-04-16</td>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>ATL</td>\n",
       "      <td>BOS</td>\n",
       "      <td>PT5H32M</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>251.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1650198060||1650213120</td>\n",
       "      <td>2022-04-17T08:21:00.000-04:00||2022-04-17T12:3...</td>\n",
       "      <td>CLT||BOS</td>\n",
       "      <td>ATL||CLT</td>\n",
       "      <td>American Airlines||American Airlines</td>\n",
       "      <td>AA||AA</td>\n",
       "      <td>Airbus A319||Airbus A320</td>\n",
       "      <td>4860||7500</td>\n",
       "      <td>228||728</td>\n",
       "      <td>coach||coach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>562e7d5dd6ecbf1509c0c19711dbdca9</td>\n",
       "      <td>2022-04-16</td>\n",
       "      <td>2022-04-17</td>\n",
       "      <td>ATL</td>\n",
       "      <td>BOS</td>\n",
       "      <td>PT6H38M</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>251.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1650198060||1650217080</td>\n",
       "      <td>2022-04-17T08:21:00.000-04:00||2022-04-17T13:3...</td>\n",
       "      <td>CLT||BOS</td>\n",
       "      <td>ATL||CLT</td>\n",
       "      <td>American Airlines||American Airlines</td>\n",
       "      <td>AA||AA</td>\n",
       "      <td>Airbus A319||Boeing 737-800</td>\n",
       "      <td>4860||8280</td>\n",
       "      <td>228||728</td>\n",
       "      <td>coach||coach</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              legId  searchDate  flightDate startingAirport  \\\n",
       "0  9ca0e81111c683bec1012473feefd28f  2022-04-16  2022-04-17             ATL   \n",
       "1  98685953630e772a098941b71906592b  2022-04-16  2022-04-17             ATL   \n",
       "2  98d90cbc32bfbb05c2fc32897c7c1087  2022-04-16  2022-04-17             ATL   \n",
       "3  969a269d38eae583f455486fa90877b4  2022-04-16  2022-04-17             ATL   \n",
       "4  980370cf27c89b40d2833a1d5afc9751  2022-04-16  2022-04-17             ATL   \n",
       "5  79eda9f841e226a1e2121d74211e595c  2022-04-16  2022-04-17             ATL   \n",
       "6  9335fae376c38bb61263281779f469ec  2022-04-16  2022-04-17             ATL   \n",
       "7  3904bf87f2d1daf334f1ae7e3b876028  2022-04-16  2022-04-17             ATL   \n",
       "8  d93988734c44a3c075d9efe373352507  2022-04-16  2022-04-17             ATL   \n",
       "9  562e7d5dd6ecbf1509c0c19711dbdca9  2022-04-16  2022-04-17             ATL   \n",
       "\n",
       "  destinationAirport travelDuration  isBasicEconomy  isRefundable  isNonStop  \\\n",
       "0                BOS        PT2H29M           False         False       True   \n",
       "1                BOS        PT2H30M           False         False       True   \n",
       "2                BOS        PT2H30M           False         False       True   \n",
       "3                BOS        PT2H32M           False         False       True   \n",
       "4                BOS        PT2H34M           False         False       True   \n",
       "5                BOS        PT2H38M           False         False       True   \n",
       "6                BOS        PT4H12M           False         False      False   \n",
       "7                BOS        PT5H18M           False         False      False   \n",
       "8                BOS        PT5H32M           False         False      False   \n",
       "9                BOS        PT6H38M           False         False      False   \n",
       "\n",
       "   totalFare  ...  segmentsArrivalTimeEpochSeconds  \\\n",
       "0      248.6  ...                       1650223560   \n",
       "1      248.6  ...                       1650200400   \n",
       "2      248.6  ...                       1650218700   \n",
       "3      248.6  ...                       1650227460   \n",
       "4      248.6  ...                       1650213180   \n",
       "5      248.6  ...                       1650216180   \n",
       "6      251.1  ...           1650203400||1650213120   \n",
       "7      251.1  ...           1650203400||1650217080   \n",
       "8      251.1  ...           1650198060||1650213120   \n",
       "9      251.1  ...           1650198060||1650217080   \n",
       "\n",
       "                              segmentsArrivalTimeRaw  \\\n",
       "0                      2022-04-17T15:26:00.000-04:00   \n",
       "1                      2022-04-17T09:00:00.000-04:00   \n",
       "2                      2022-04-17T14:05:00.000-04:00   \n",
       "3                      2022-04-17T16:31:00.000-04:00   \n",
       "4                      2022-04-17T12:33:00.000-04:00   \n",
       "5                      2022-04-17T13:23:00.000-04:00   \n",
       "6  2022-04-17T09:50:00.000-04:00||2022-04-17T12:3...   \n",
       "7  2022-04-17T09:50:00.000-04:00||2022-04-17T13:3...   \n",
       "8  2022-04-17T08:21:00.000-04:00||2022-04-17T12:3...   \n",
       "9  2022-04-17T08:21:00.000-04:00||2022-04-17T13:3...   \n",
       "\n",
       "  segmentsArrivalAirportCode segmentsDepartureAirportCode  \\\n",
       "0                        BOS                          ATL   \n",
       "1                        BOS                          ATL   \n",
       "2                        BOS                          ATL   \n",
       "3                        BOS                          ATL   \n",
       "4                        BOS                          ATL   \n",
       "5                        BOS                          ATL   \n",
       "6                   CLT||BOS                     ATL||CLT   \n",
       "7                   CLT||BOS                     ATL||CLT   \n",
       "8                   CLT||BOS                     ATL||CLT   \n",
       "9                   CLT||BOS                     ATL||CLT   \n",
       "\n",
       "                    segmentsAirlineName segmentsAirlineCode  \\\n",
       "0                                 Delta                  DL   \n",
       "1                                 Delta                  DL   \n",
       "2                                 Delta                  DL   \n",
       "3                                 Delta                  DL   \n",
       "4                                 Delta                  DL   \n",
       "5                       JetBlue Airways                  B6   \n",
       "6  American Airlines||American Airlines              AA||AA   \n",
       "7  American Airlines||American Airlines              AA||AA   \n",
       "8  American Airlines||American Airlines              AA||AA   \n",
       "9  American Airlines||American Airlines              AA||AA   \n",
       "\n",
       "  segmentsEquipmentDescription segmentsDurationInSeconds segmentsDistance  \\\n",
       "0                  Airbus A321                      8940              947   \n",
       "1                  Airbus A321                      9000              947   \n",
       "2               Boeing 757-200                      9000              947   \n",
       "3                  Airbus A321                      9120              947   \n",
       "4                  Airbus A321                      9240              947   \n",
       "5                         None                      9480              947   \n",
       "6     Airbus A320||Airbus A320                5400||7500         228||728   \n",
       "7  Airbus A320||Boeing 737-800                5400||8280         228||728   \n",
       "8     Airbus A319||Airbus A320                4860||7500         228||728   \n",
       "9  Airbus A319||Boeing 737-800                4860||8280         228||728   \n",
       "\n",
       "  segmentsCabinCode  \n",
       "0             coach  \n",
       "1             coach  \n",
       "2             coach  \n",
       "3             coach  \n",
       "4             coach  \n",
       "5             coach  \n",
       "6      coach||coach  \n",
       "7      coach||coach  \n",
       "8      coach||coach  \n",
       "9      coach||coach  \n",
       "\n",
       "[10 rows x 23 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2697f5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported the first 1000 records to C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\exported_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Specify the number of records you want to export (e.g., 1000 records)\n",
    "export_count = 1000\n",
    "\n",
    "# Create a DataFrame containing the first 1000 records\n",
    "export_df = df.head(export_count)\n",
    "\n",
    "# Define the path for the CSV file where you want to export the data\n",
    "csv_file = r'C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\exported_data.csv'\n",
    "\n",
    "# Export the data to a CSV file\n",
    "export_df.to_csv(csv_file, index=False)  # Set index=False to exclude the DataFrame index column\n",
    "\n",
    "print(f'Exported the first {export_count} records to {csv_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fd5695b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null value counts in each column:\n",
      "legId                                    0\n",
      "searchDate                               0\n",
      "flightDate                               0\n",
      "startingAirport                          0\n",
      "destinationAirport                       0\n",
      "travelDuration                           0\n",
      "isBasicEconomy                           0\n",
      "isRefundable                             0\n",
      "isNonStop                                0\n",
      "totalFare                                0\n",
      "totalTravelDistance                   4524\n",
      "segmentsDepartureTimeEpochSeconds        0\n",
      "segmentsDepartureTimeRaw                 0\n",
      "segmentsArrivalTimeEpochSeconds          0\n",
      "segmentsArrivalTimeRaw                   0\n",
      "segmentsArrivalAirportCode               0\n",
      "segmentsDepartureAirportCode             0\n",
      "segmentsAirlineName                      0\n",
      "segmentsAirlineCode                      0\n",
      "segmentsEquipmentDescription         19965\n",
      "segmentsDurationInSeconds                0\n",
      "segmentsDistance                       486\n",
      "segmentsCabinCode                        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in each column\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "# Display the number of null values in each column\n",
    "print(\"Null value counts in each column:\")\n",
    "print(null_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e933f775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed empty string counts in each column:\n",
      "   Column1  Column2\n",
      "0        6        5\n",
      "1        1        1\n",
      "2        1        1\n",
      "3        6        1\n",
      "4        8        5\n"
     ]
    }
   ],
   "source": [
    "# Sample DataFrame\n",
    "\n",
    "# Define a function to count empty strings after trimming\n",
    "def count_trimmed_empty_strings(column):\n",
    "    return column.apply(lambda x: x.strip().count(''))\n",
    "\n",
    "# Apply the function to each column\n",
    "empty_string_counts = df.apply(count_trimmed_empty_strings)\n",
    "\n",
    "# Display the number of trimmed empty strings in each column\n",
    "print(\"Trimmed empty string counts in each column:\")\n",
    "print(empty_string_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63cb8615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated records:\n",
      "Empty DataFrame\n",
      "Columns: [legId, searchDate, flightDate, startingAirport, destinationAirport, travelDuration, isBasicEconomy, isRefundable, isNonStop, totalFare, totalTravelDistance, segmentsDepartureTimeEpochSeconds, segmentsDepartureTimeRaw, segmentsArrivalTimeEpochSeconds, segmentsArrivalTimeRaw, segmentsArrivalAirportCode, segmentsDepartureAirportCode, segmentsAirlineName, segmentsAirlineCode, segmentsEquipmentDescription, segmentsDurationInSeconds, segmentsDistance, segmentsCabinCode]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicated records based on all columns\n",
    "duplicates = df[df.duplicated()]\n",
    "\n",
    "# Display the duplicated records\n",
    "print(\"Duplicated records:\")\n",
    "print(duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1755c6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record counts by search date:\n",
      "    searchDate  record_count\n",
      "0   2022-04-16         87657\n",
      "1   2022-04-17        536269\n",
      "2   2022-04-18        457449\n",
      "3   2022-04-19        526764\n",
      "4   2022-04-20        331144\n",
      "5   2022-04-21        522447\n",
      "6   2022-04-22        511658\n",
      "7   2022-04-23        524379\n",
      "8   2022-04-24        232588\n",
      "9   2022-04-25        529018\n",
      "10  2022-04-26        301682\n",
      "11  2022-04-27        462260\n",
      "12  2022-04-28        530492\n",
      "13  2022-04-29        531353\n",
      "14  2022-04-30        532206\n",
      "15  2022-05-01        527933\n",
      "16  2022-05-02        540011\n",
      "17  2022-05-03        531400\n",
      "18  2022-05-04        504047\n",
      "19  2022-05-05        526935\n",
      "20  2022-05-06        526760\n",
      "21  2022-05-07        522793\n",
      "22  2022-05-08        518705\n",
      "23  2022-05-09        525063\n",
      "24  2022-05-10        150199\n",
      "25  2022-05-11        112312\n",
      "26  2022-05-12        522779\n",
      "27  2022-05-13        521824\n",
      "28  2022-05-14        174054\n",
      "29  2022-05-17         35129\n",
      "30  2022-05-18        526647\n",
      "31  2022-05-19        136042\n"
     ]
    }
   ],
   "source": [
    "# Group the data by the 'search_date' column and count records in each group\n",
    "date_counts = df.groupby('searchDate').size().reset_index(name='record_count')\n",
    "\n",
    "# Display the counts\n",
    "print(\"Record counts by search date:\")\n",
    "print(date_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e55af531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record counts by flight date:\n",
      "    flightDate  record_count\n",
      "0   2022-04-17          8258\n",
      "1   2022-04-18         16524\n",
      "2   2022-04-19         30702\n",
      "3   2022-04-20         41078\n",
      "4   2022-04-21         45888\n",
      "..         ...           ...\n",
      "87  2022-07-13          9244\n",
      "88  2022-07-14          9512\n",
      "89  2022-07-15          9153\n",
      "90  2022-07-16          7941\n",
      "91  2022-07-17          7905\n",
      "\n",
      "[92 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Group the data by the 'search_date' column and count records in each group\n",
    "date_counts = df.groupby('flightDate').size().reset_index(name='record_count')\n",
    "\n",
    "# Display the counts\n",
    "print(\"Record counts by flight date:\")\n",
    "print(date_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38616dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns and Data Types:\n",
      "legId                                 object\n",
      "searchDate                            object\n",
      "flightDate                            object\n",
      "startingAirport                       object\n",
      "destinationAirport                    object\n",
      "travelDuration                        object\n",
      "isBasicEconomy                          bool\n",
      "isRefundable                            bool\n",
      "isNonStop                               bool\n",
      "totalFare                            float64\n",
      "totalTravelDistance                  float64\n",
      "segmentsDepartureTimeEpochSeconds     object\n",
      "segmentsDepartureTimeRaw              object\n",
      "segmentsArrivalTimeEpochSeconds       object\n",
      "segmentsArrivalTimeRaw                object\n",
      "segmentsArrivalAirportCode            object\n",
      "segmentsDepartureAirportCode          object\n",
      "segmentsAirlineName                   object\n",
      "segmentsAirlineCode                   object\n",
      "segmentsEquipmentDescription          object\n",
      "segmentsDurationInSeconds             object\n",
      "segmentsDistance                      object\n",
      "segmentsCabinCode                     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# List columns and their data types\n",
    "column_data_types = df.dtypes\n",
    "\n",
    "# Display the column names and their data types\n",
    "print(\"Columns and Data Types:\")\n",
    "print(column_data_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "1134e1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in merged_batch_0.parquet: 1346481\n",
      "Number of records in merged_batch_1.parquet: 1335784\n",
      "Number of records in merged_batch_2.parquet: 1206900\n",
      "Number of records in merged_batch_3.parquet: 1180923\n",
      "Number of records in merged_batch_4.parquet: 988544\n",
      "Number of records in merged_batch_5.parquet: 970579\n",
      "Number of records in merged_batch_6.parquet: 1803984\n",
      "Number of records in merged_batch_7.parquet: 1386163\n",
      "Number of records in merged_batch_8.parquet: 959749\n",
      "Number of records in merged_batch_9.parquet: 1231534\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Define the list of Parquet files\n",
    "parquet_files = [\n",
    "    'merged_batch_0.parquet',\n",
    "    'merged_batch_1.parquet',\n",
    "    'merged_batch_2.parquet',\n",
    "    'merged_batch_3.parquet',\n",
    "    'merged_batch_4.parquet',\n",
    "    'merged_batch_5.parquet',\n",
    "    'merged_batch_6.parquet',\n",
    "    'merged_batch_7.parquet',\n",
    "    'merged_batch_8.parquet',\n",
    "    'merged_batch_9.parquet',\n",
    "]\n",
    "\n",
    "# Iterate through the list of Parquet files and count the number of records in each file\n",
    "for file in parquet_files:\n",
    "    table = pq.read_table(file)\n",
    "    num_records = len(table)\n",
    "    print(f\"Number of records in {file}: {num_records}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "773c427f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 959749\n",
      "Number of columns: 23\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Path to the merged Parquet file\n",
    "#parquet_file = r'C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\merged_all.parquet'\n",
    "parquet_file = r'C:\\Users\\sudar\\Desktop\\AdvML\\AT3\\merged_batch_8.parquet'\n",
    "\n",
    "# Read the Parquet file into a PyArrow table\n",
    "table = pq.read_table(parquet_file)\n",
    "\n",
    "# Convert the PyArrow table to a Pandas DataFrame\n",
    "df = table.to_pandas()\n",
    "\n",
    "num_rows, num_cols = df.shape\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_cols}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "26606057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define conditions and corresponding values using a dictionary\n",
    "conditions = {\n",
    "    \"business\": 1,\n",
    "    \"business||business\": 1,\n",
    "    \"coach\": 2,\n",
    "    \"coach||coach\": 2,\n",
    "    \"coach||coach||coach\": 2,\n",
    "    \"first\": 3,\n",
    "    \"first||first\": 3,\n",
    "    \"first||first||first\": 3,\n",
    "    \"business||business||coach\": 5,\n",
    "    \"business||coach\": 5,\n",
    "    \"business||coach||business\": 5,\n",
    "    \"business||coach||coach\": 5,\n",
    "    \"business||first\": 5,\n",
    "    \"business||first||first\": 5,\n",
    "    \"coach||business\": 5,\n",
    "    \"coach||business||business\": 5,\n",
    "    \"coach||business||coach\": 5,\n",
    "    \"coach||business||first\": 5,\n",
    "    \"coach||coach||business\": 5,\n",
    "    \"coach||coach||business||coach\": 5,\n",
    "    \"coach||coach||coach||coach\": 5,\n",
    "    \"coach||coach||coach||first\": 5,\n",
    "    \"coach||coach||coach||premium coach\": 5,\n",
    "    \"coach||coach||first\": 5,\n",
    "    \"coach||coach||first||coach\": 5,\n",
    "    \"coach||coach||first||first\": 5,\n",
    "    \"coach||coach||premium coach\": 5,\n",
    "    \"coach||coach||premium coach||coach\": 5,\n",
    "    \"coach||coach||premium coach||premium coach\": 5,\n",
    "    \"coach||first\": 5,\n",
    "    \"coach||first||coach\": 5,\n",
    "    \"coach||first||first\": 5,\n",
    "    \"coach||premium coach\": 5,\n",
    "    \"coach||premium coach||coach\": 5,\n",
    "    \"coach||premium coach||premium coach\": 5,\n",
    "    \"first||business\": 5,\n",
    "    \"first||coach\": 5,\n",
    "    \"first||coach||business\": 5,\n",
    "    \"first||coach||coach\": 5,\n",
    "    \"first||coach||coach||coach\": 5,\n",
    "    \"first||coach||first\": 5,\n",
    "    \"first||first||coach\": 5,\n",
    "    \"first||first||coach||coach\": 5,\n",
    "    \"premium coach||business||coach\": 5,\n",
    "    \"premium coach||coach\": 5,\n",
    "    \"premium coach||coach||coach\": 5,\n",
    "    \"premium coach||coach||coach||coach\": 5,\n",
    "    \"premium coach||first\": 5,\n",
    "    \"premium coach||premium coach||coach\": 5,\n",
    "    \"premium coach\": 4,\n",
    "    \"premium coach||premium coach\": 4,\n",
    "    \"premium coach||premium coach||premium coach\": 4\n",
    "}\n",
    "\n",
    "# Use numpy.select to set 'segmentsCabinCodeNum' based on conditions\n",
    "df['segmentsCabinCodeNum'] = np.select([df['segmentsCabinCode'] == cond for cond in conditions.keys()], list(conditions.values()), default=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1744038c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coach' 'coach||coach' 'coach||coach||coach' 'first||coach'\n",
      " 'premium coach' 'premium coach||coach' 'premium coach||premium coach'\n",
      " 'coach||premium coach' 'coach||coach||coach||coach' 'coach||first||first'\n",
      " 'coach||coach||business' 'first||first||coach' 'coach||first' 'first'\n",
      " 'coach||coach||premium coach' 'business||business' 'coach||first||coach'\n",
      " 'first||first' 'coach||coach||first' 'coach||premium coach||coach'\n",
      " 'coach||business' 'first||first||first' 'business||coach||coach'\n",
      " 'business||coach' 'business||business||coach' 'coach||business||business'\n",
      " 'business||coach||business' 'coach||business||coach'\n",
      " 'coach||premium coach||premium coach' 'first||coach||coach'\n",
      " 'first||coach||first' 'coach||coach||first||coach'\n",
      " 'premium coach||coach||coach' 'coach||coach||coach||first'\n",
      " 'coach||coach||first||first']\n",
      "segmentsCabinCodeNum\n",
      "2    950672\n",
      "5      8184\n",
      "3       610\n",
      "4       260\n",
      "1        23\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get the distinct values in the 'segmentsCabinCode' column\n",
    "distinct_values = df['segmentsCabinCode'].unique()\n",
    "\n",
    "print(distinct_values)\n",
    "\n",
    "value_counts = df['segmentsCabinCodeNum'].value_counts()\n",
    "\n",
    "print(value_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b411f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "from dateutil.parser import parse\n",
    "from dateutil import parser\n",
    "\n",
    "# Calculate correlation scores between columns and 'totalFare'\n",
    "\n",
    "# Convert 'searchDate' to Unix timestamps\n",
    "if 'searchDate' in df.columns: df['searchDateNum'] = df['searchDate'].apply(lambda x: datetime.timestamp(datetime.strptime(str(x), '%Y-%m-%d')) if not isinstance(x, float) else x)\n",
    "if 'flightDate' in df.columns: df['flightDateNum'] = df['flightDate'].apply(lambda x: datetime.timestamp(datetime.strptime(str(x), '%Y-%m-%d')) if not isinstance(x, float) else x)\n",
    "\n",
    "if 'startingAirport' in df.columns: df['startingAirportNum'], _ = pd.factorize(df['startingAirport'])\n",
    "if 'destinationAirport' in df.columns: df['destinationAirportNum'], _ = pd.factorize(df['destinationAirport'])\n",
    "if 'segmentsDepartureTimeEpochSeconds' in df.columns: \n",
    "    df['segmentsDepartureTimeEpochSecondsNum'], _ = pd.factorize(df['segmentsDepartureTimeEpochSeconds'])\n",
    "if 'segmentsDepartureTimeRaw' in df.columns: \n",
    "    df['segmentsDepartureTimeRawNum'], _ = pd.factorize(df['segmentsDepartureTimeRaw'])\n",
    "if 'segmentsArrivalTimeEpochSeconds' in df.columns: \n",
    "    df['segmentsArrivalTimeEpochSecondsNum'], _ = pd.factorize(df['segmentsArrivalTimeEpochSeconds'])    \n",
    "if 'segmentsArrivalTimeRaw' in df.columns: \n",
    "    df['segmentsArrivalTimeRawNum'], _ = pd.factorize(df['segmentsArrivalTimeRaw'])       \n",
    "if 'segmentsArrivalAirportCode' in df.columns: \n",
    "    df['segmentsArrivalAirportCodeNum'], _ = pd.factorize(df['segmentsArrivalAirportCode'])\n",
    "if 'segmentsDepartureAirportCode' in df.columns: \n",
    "    df['segmentsDepartureAirportCodeNum'], _ = pd.factorize(df['segmentsDepartureAirportCode'])    \n",
    "if 'segmentsAirlineName' in df.columns: \n",
    "    df['segmentsAirlineNameNum'], _ = pd.factorize(df['segmentsAirlineName'])\n",
    "if 'segmentsAirlineCode' in df.columns: \n",
    "    df['segmentsAirlineCodeNum'], _ = pd.factorize(df['segmentsAirlineCode'])\n",
    "if 'segmentsEquipmentDescription' in df.columns: \n",
    "    df['segmentsEquipmentDescriptionNum'], _ = pd.factorize(df['segmentsEquipmentDescription'])    \n",
    "if 'segmentsDurationInSeconds' in df.columns: \n",
    "    df['segmentsDurationInSecondsNum'], _ = pd.factorize(df['segmentsDurationInSeconds']) \n",
    "if 'segmentsDistance' in df.columns: \n",
    "    df['segmentsDistanceNum'], _ = pd.factorize(df['segmentsDistance'])     \n",
    "#if 'segmentsCabinCode' in df.columns: \n",
    "#    df['segmentsCabinCodeNum'], _ = pd.factorize(df['segmentsCabinCode'])    \n",
    "    \n",
    "    \n",
    "# Define a function to convert duration strings to 'x.xx' format\n",
    "def convert_duration(duration_str):\n",
    "    match = re.search(r'(\\d+)H(\\d+)M', duration_str)\n",
    "    if match:\n",
    "        hours, minutes = map(int, match.groups())\n",
    "        total_hours = hours + minutes / 60.0\n",
    "        return '{:.2f}'.format(total_hours)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the conversion function to the 'travelDuration' column\n",
    "if 'travelDuration' in df.columns: df['travelDurationNum'] = df['travelDuration'].apply(convert_duration)\n",
    "    \n",
    "    \n",
    "##########################################################################################\n",
    "\n",
    "df['segmentsDepartureTimeRawStr'] = df['segmentsDepartureTimeRaw'].astype(str)\n",
    "df['segmentsDepartureTimeRawStr'] = df['segmentsDepartureTimeRawStr'].str.split(r'\\|\\|').str[0]\n",
    "\n",
    "df['segmentsDepartureTimeRawStr'] = df['segmentsDepartureTimeRawStr'].apply(lambda x: parser.parse(x) if pd.notna(x) else None)\n",
    "df['day_of_week'] = df['segmentsDepartureTimeRawStr'].apply(lambda x: x.strftime('%A') if x else None)\n",
    "\n",
    "df['departure_month'] = df['segmentsDepartureTimeRawStr'].apply(lambda x: pd.to_datetime(x).month if pd.notna(x) else None)\n",
    "df['departure_time_bin'] = df['segmentsDepartureTimeRawStr'].apply(lambda x: (x.hour * 60 + x.minute) // 120)\n",
    "df['departure_month'] = df['segmentsDepartureTimeRawStr'].apply(lambda x: pd.to_datetime(x).month if pd.notna(x) else None)\n",
    "    \n",
    "##########################################################################################\n",
    "\n",
    "# Define a mapping from day names to numbers\n",
    "day_to_number = {\n",
    "    'Monday': 1,\n",
    "    'Tuesday': 2,\n",
    "    'Wednesday': 3,\n",
    "    'Thursday': 4,\n",
    "    'Friday': 5,\n",
    "    'Saturday': 6,\n",
    "    'Sunday': 7\n",
    "}\n",
    "\n",
    "# Convert day of the week to a number\n",
    "df['departure_day_of_week'] = df['day_of_week'].map(day_to_number)\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "if 'day_of_week' in df.columns: df = df.drop('day_of_week', axis=1)\n",
    "if 'segmentsDepartureTimeRawStr' in df.columns: df = df.drop('segmentsDepartureTimeRawStr', axis=1)\n",
    "\n",
    "if 'legId' in df.columns: df = df.drop('legId', axis=1)\n",
    "if 'searchDate' in df.columns: df = df.drop('searchDate', axis=1)\n",
    "if 'flightDate' in df.columns: df = df.drop('flightDate', axis=1)\n",
    "if 'startingAirport' in df.columns: df = df.drop('startingAirport', axis=1)\n",
    "if 'destinationAirport' in df.columns: df = df.drop('destinationAirport', axis=1)\n",
    "if 'travelDuration' in df.columns: df = df.drop('travelDuration', axis=1) \n",
    "if 'segmentsDepartureTimeEpochSeconds' in df.columns: df = df.drop('segmentsDepartureTimeEpochSeconds', axis=1)   \n",
    "if 'segmentsDepartureTimeRaw' in df.columns: df = df.drop('segmentsDepartureTimeRaw', axis=1)\n",
    "if 'segmentsArrivalTimeEpochSeconds' in df.columns: df = df.drop('segmentsArrivalTimeEpochSeconds', axis=1)    \n",
    "if 'segmentsArrivalTimeRaw' in df.columns: df = df.drop('segmentsArrivalTimeRaw', axis=1)    \n",
    "if 'segmentsArrivalAirportCode' in df.columns: df = df.drop('segmentsArrivalAirportCode', axis=1) \n",
    "if 'segmentsDepartureAirportCode' in df.columns: df = df.drop('segmentsDepartureAirportCode', axis=1)    \n",
    "if 'segmentsAirlineName' in df.columns: df = df.drop('segmentsAirlineName', axis=1)\n",
    "if 'segmentsAirlineCode' in df.columns: df = df.drop('segmentsAirlineCode', axis=1)    \n",
    "if 'segmentsEquipmentDescription' in df.columns: df = df.drop('segmentsEquipmentDescription', axis=1)      \n",
    "if 'segmentsDurationInSeconds' in df.columns: df = df.drop('segmentsDurationInSeconds', axis=1)  \n",
    "if 'segmentsDistance' in df.columns: df = df.drop('segmentsDistance', axis=1)      \n",
    "if 'segmentsCabinCode' in df.columns: df = df.drop('segmentsCabinCode', axis=1)       \n",
    "    \n",
    "      \n",
    "#correlation_scores = df.corr()['totalFare']\n",
    "\n",
    "# Create a DataFrame from the correlation scores\n",
    "#correlation_df = pd.DataFrame({'Feature': correlation_scores.index, 'Correlation Score': correlation_scores.values})\n",
    "\n",
    "# Sort the DataFrame by 'Correlation Score' in descending order\n",
    "#correlation_df = correlation_df.sort_values(by='Correlation Score', ascending=False)\n",
    "\n",
    "# Print the DataFrame in a tabular format\n",
    "#print(\"Correlation Scores with 'totalFare' (High to Low):\")\n",
    "#print(correlation_df)    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b2b88b79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns and Data Types:\n",
      "isBasicEconomy                             bool\n",
      "isRefundable                               bool\n",
      "isNonStop                                  bool\n",
      "totalFare                               float64\n",
      "totalTravelDistance                     float64\n",
      "segmentsCabinCodeNum                      int32\n",
      "searchDateNum                           float64\n",
      "flightDateNum                           float64\n",
      "startingAirportNum                        int64\n",
      "destinationAirportNum                     int64\n",
      "segmentsDepartureTimeEpochSecondsNum      int64\n",
      "segmentsDepartureTimeRawNum               int64\n",
      "segmentsArrivalTimeEpochSecondsNum        int64\n",
      "segmentsArrivalTimeRawNum                 int64\n",
      "segmentsArrivalAirportCodeNum             int64\n",
      "segmentsDepartureAirportCodeNum           int64\n",
      "segmentsAirlineNameNum                    int64\n",
      "segmentsAirlineCodeNum                    int64\n",
      "segmentsEquipmentDescriptionNum           int64\n",
      "segmentsDurationInSecondsNum              int64\n",
      "segmentsDistanceNum                       int64\n",
      "travelDurationNum                       float64\n",
      "departure_month                           int64\n",
      "departure_time_bin                        int64\n",
      "departure_day_of_week                     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# List columns and their data types\n",
    "column_data_types = df.dtypes\n",
    "\n",
    "# Display the column names and their data types\n",
    "print(\"Columns and Data Types:\")\n",
    "print(column_data_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "4a9d0bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null value counts in each column:\n",
      "isBasicEconomy                          0\n",
      "isRefundable                            0\n",
      "isNonStop                               0\n",
      "totalFare                               0\n",
      "totalTravelDistance                     0\n",
      "segmentsCabinCodeNum                    0\n",
      "searchDateNum                           0\n",
      "flightDateNum                           0\n",
      "startingAirportNum                      0\n",
      "destinationAirportNum                   0\n",
      "segmentsDepartureTimeEpochSecondsNum    0\n",
      "segmentsDepartureTimeRawNum             0\n",
      "segmentsArrivalTimeEpochSecondsNum      0\n",
      "segmentsArrivalTimeRawNum               0\n",
      "segmentsArrivalAirportCodeNum           0\n",
      "segmentsDepartureAirportCodeNum         0\n",
      "segmentsAirlineNameNum                  0\n",
      "segmentsAirlineCodeNum                  0\n",
      "segmentsEquipmentDescriptionNum         0\n",
      "segmentsDurationInSecondsNum            0\n",
      "segmentsDistanceNum                     0\n",
      "travelDurationNum                       0\n",
      "departure_month                         0\n",
      "departure_time_bin                      0\n",
      "departure_day_of_week                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in each column\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "# Display the number of null values in each column\n",
    "print(\"Null value counts in each column:\")\n",
    "print(null_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "becfeeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    startingAirportNum  destinationAirportNum  averageTravelDuration  \\\n",
      "0                    0                      0               5.009703   \n",
      "1                    0                      1               5.835933   \n",
      "2                    0                      2               5.755659   \n",
      "3                    0                      3               8.135166   \n",
      "4                    0                      4               6.110604   \n",
      "5                    0                      5               6.755580   \n",
      "6                    0                      6               4.860255   \n",
      "7                    0                      7               6.913987   \n",
      "8                    0                      8               3.580476   \n",
      "9                    0                      9               9.323770   \n",
      "10                   0                     10               4.925771   \n",
      "11                   0                     11              13.180665   \n",
      "12                   0                     12               6.805600   \n",
      "13                   0                     13               6.198640   \n",
      "14                   0                     14              11.240872   \n",
      "15                   1                      0              11.150172   \n",
      "16                   1                      1              12.193321   \n",
      "17                   1                      2              12.419861   \n",
      "18                   1                      3               7.879551   \n",
      "19                   1                      4              10.831777   \n",
      "20                   1                      5              11.292585   \n",
      "21                   1                      6              11.302121   \n",
      "22                   1                      7              11.367206   \n",
      "23                   1                      8              10.266597   \n",
      "24                   1                      9               7.741897   \n",
      "25                   1                     10              13.562752   \n",
      "26                   1                     12               9.687846   \n",
      "27                   1                     13              12.201070   \n",
      "28                   1                     14               7.233795   \n",
      "29                   1                     15              13.368342   \n",
      "30                   2                      0               5.936949   \n",
      "31                   2                      1               3.814843   \n",
      "32                   2                      2               5.560995   \n",
      "33                   2                      3               5.231175   \n",
      "34                   2                      4               6.239652   \n",
      "35                   2                      5               2.488414   \n",
      "36                   2                      6               4.104420   \n",
      "37                   2                      7               5.562943   \n",
      "38                   2                      8               6.097222   \n",
      "39                   2                      9               7.326782   \n",
      "40                   2                     10               3.413666   \n",
      "41                   2                     11              10.387426   \n",
      "42                   2                     13               5.030400   \n",
      "43                   2                     14               7.476101   \n",
      "44                   2                     15               6.232489   \n",
      "\n",
      "    averageTotalTravelDistance  \n",
      "0                  1016.697852  \n",
      "1                  1207.113814  \n",
      "2                   949.404913  \n",
      "3                  1904.494724  \n",
      "4                  1154.502522  \n",
      "5                  1164.800813  \n",
      "6                  1063.155633  \n",
      "7                  1136.863299  \n",
      "8                  1114.215184  \n",
      "9                  1949.220502  \n",
      "10                 1156.762775  \n",
      "11                 2424.409610  \n",
      "12                 1337.170636  \n",
      "13                 1045.688328  \n",
      "14                 2889.646624  \n",
      "15                 1919.009685  \n",
      "16                 2841.478029  \n",
      "17                 2439.933412  \n",
      "18                 1165.888630  \n",
      "19                 1298.307986  \n",
      "20                 2013.713313  \n",
      "21                 2541.001236  \n",
      "22                 2772.505675  \n",
      "23                 2794.496170  \n",
      "24                 1073.996913  \n",
      "25                 2977.907609  \n",
      "26                 1553.134685  \n",
      "27                 2510.336843  \n",
      "28                  887.619095  \n",
      "29                 2935.506244  \n",
      "30                  809.633389  \n",
      "31                  850.005471  \n",
      "32                  929.011752  \n",
      "33                 1176.740096  \n",
      "34                  901.921517  \n",
      "35                  431.718396  \n",
      "36                  706.909062  \n",
      "37                  862.585891  \n",
      "38                  935.860695  \n",
      "39                 1754.915303  \n",
      "40                  715.018084  \n",
      "41                 1736.945578  \n",
      "42                  795.530042  \n",
      "43                 2074.764959  \n",
      "44                 1444.932904  \n"
     ]
    }
   ],
   "source": [
    "# Group by 'startingAirportNum' and 'destinationAirportNum' and calculate the average of 'travelDurationNum' and 'totalTravelDistance'\n",
    "grouped_df = df.groupby(['startingAirportNum', 'destinationAirportNum']).agg({\n",
    "    'travelDurationNum': 'mean',\n",
    "    'totalTravelDistance': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "grouped_df = grouped_df.rename(columns={\n",
    "    'travelDurationNum': 'averageTravelDuration',\n",
    "    'totalTravelDistance': 'averageTotalTravelDistance'\n",
    "})\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(grouped_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "e1f24472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isBasicEconomy</th>\n",
       "      <th>isRefundable</th>\n",
       "      <th>isNonStop</th>\n",
       "      <th>totalFare</th>\n",
       "      <th>totalTravelDistance</th>\n",
       "      <th>segmentsCabinCodeNum</th>\n",
       "      <th>searchDateNum</th>\n",
       "      <th>flightDateNum</th>\n",
       "      <th>startingAirportNum</th>\n",
       "      <th>destinationAirportNum</th>\n",
       "      <th>...</th>\n",
       "      <th>segmentsDepartureAirportCodeNum</th>\n",
       "      <th>segmentsAirlineNameNum</th>\n",
       "      <th>segmentsAirlineCodeNum</th>\n",
       "      <th>segmentsEquipmentDescriptionNum</th>\n",
       "      <th>segmentsDurationInSecondsNum</th>\n",
       "      <th>segmentsDistanceNum</th>\n",
       "      <th>travelDurationNum</th>\n",
       "      <th>departure_month</th>\n",
       "      <th>departure_time_bin</th>\n",
       "      <th>departure_day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>76.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.652018e+09</td>\n",
       "      <td>1.656684e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.97</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>80.98</td>\n",
       "      <td>596.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.652018e+09</td>\n",
       "      <td>1.656684e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.07</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>127.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.652018e+09</td>\n",
       "      <td>1.656684e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.73</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>127.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.652018e+09</td>\n",
       "      <td>1.656684e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4.48</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>138.60</td>\n",
       "      <td>596.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.652018e+09</td>\n",
       "      <td>1.656684e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.88</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>347.61</td>\n",
       "      <td>663.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.652018e+09</td>\n",
       "      <td>1.656684e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>37</td>\n",
       "      <td>77</td>\n",
       "      <td>16</td>\n",
       "      <td>3.50</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>347.61</td>\n",
       "      <td>663.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.652018e+09</td>\n",
       "      <td>1.656684e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>78</td>\n",
       "      <td>16</td>\n",
       "      <td>3.60</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>347.61</td>\n",
       "      <td>824.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.652018e+09</td>\n",
       "      <td>1.656684e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>41</td>\n",
       "      <td>79</td>\n",
       "      <td>12</td>\n",
       "      <td>3.88</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>513.60</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.652018e+09</td>\n",
       "      <td>1.656684e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>80</td>\n",
       "      <td>19</td>\n",
       "      <td>7.17</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>532.20</td>\n",
       "      <td>1874.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.652018e+09</td>\n",
       "      <td>1.656684e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>81</td>\n",
       "      <td>20</td>\n",
       "      <td>12.98</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    isBasicEconomy  isRefundable  isNonStop  totalFare  totalTravelDistance  \\\n",
       "0            False         False       True      76.59                  0.0   \n",
       "1            False         False       True      80.98                596.0   \n",
       "2            False         False      False     127.58                  0.0   \n",
       "3            False         False      False     127.58                  0.0   \n",
       "4             True         False       True     138.60                596.0   \n",
       "..             ...           ...        ...        ...                  ...   \n",
       "95           False         False      False     347.61                663.0   \n",
       "96           False         False      False     347.61                663.0   \n",
       "97           False         False      False     347.61                824.0   \n",
       "98           False         False      False     513.60               1649.0   \n",
       "99           False         False      False     532.20               1874.0   \n",
       "\n",
       "    segmentsCabinCodeNum  searchDateNum  flightDateNum  startingAirportNum  \\\n",
       "0                      2   1.652018e+09   1.656684e+09                   0   \n",
       "1                      2   1.652018e+09   1.656684e+09                   0   \n",
       "2                      2   1.652018e+09   1.656684e+09                   0   \n",
       "3                      2   1.652018e+09   1.656684e+09                   0   \n",
       "4                      2   1.652018e+09   1.656684e+09                   0   \n",
       "..                   ...            ...            ...                 ...   \n",
       "95                     2   1.652018e+09   1.656684e+09                   0   \n",
       "96                     2   1.652018e+09   1.656684e+09                   0   \n",
       "97                     2   1.652018e+09   1.656684e+09                   0   \n",
       "98                     2   1.652018e+09   1.656684e+09                   0   \n",
       "99                     2   1.652018e+09   1.656684e+09                   0   \n",
       "\n",
       "    destinationAirportNum  ...  segmentsDepartureAirportCodeNum  \\\n",
       "0                       0  ...                                0   \n",
       "1                       0  ...                                0   \n",
       "2                       0  ...                                1   \n",
       "3                       0  ...                                1   \n",
       "4                       0  ...                                0   \n",
       "..                    ...  ...                              ...   \n",
       "95                      2  ...                                1   \n",
       "96                      2  ...                                1   \n",
       "97                      2  ...                                9   \n",
       "98                      2  ...                                3   \n",
       "99                      2  ...                                4   \n",
       "\n",
       "    segmentsAirlineNameNum  segmentsAirlineCodeNum  \\\n",
       "0                        0                       0   \n",
       "1                        1                       1   \n",
       "2                        2                       2   \n",
       "3                        2                       2   \n",
       "4                        3                       3   \n",
       "..                     ...                     ...   \n",
       "95                       9                       9   \n",
       "96                       9                       9   \n",
       "97                      10                      10   \n",
       "98                       8                       8   \n",
       "99                       8                       8   \n",
       "\n",
       "    segmentsEquipmentDescriptionNum  segmentsDurationInSecondsNum  \\\n",
       "0                                 0                             0   \n",
       "1                                -1                             1   \n",
       "2                                 1                             2   \n",
       "3                                 1                             2   \n",
       "4                                 2                             3   \n",
       "..                              ...                           ...   \n",
       "95                               37                            77   \n",
       "96                               26                            78   \n",
       "97                               41                            79   \n",
       "98                               34                            80   \n",
       "99                               42                            81   \n",
       "\n",
       "    segmentsDistanceNum  travelDurationNum  departure_month  \\\n",
       "0                    -1               1.97                7   \n",
       "1                     0               2.07                7   \n",
       "2                     1               3.73                7   \n",
       "3                     1               4.48                7   \n",
       "4                     0               1.88                7   \n",
       "..                  ...                ...              ...   \n",
       "95                   16               3.50                7   \n",
       "96                   16               3.60                7   \n",
       "97                   12               3.88                7   \n",
       "98                   19               7.17                7   \n",
       "99                   20              12.98                7   \n",
       "\n",
       "    departure_time_bin  departure_day_of_week  \n",
       "0                    3                      6  \n",
       "1                    3                      6  \n",
       "2                    3                      6  \n",
       "3                    3                      6  \n",
       "4                    3                      6  \n",
       "..                 ...                    ...  \n",
       "95                   7                      6  \n",
       "96                   7                      6  \n",
       "97                  10                      6  \n",
       "98                   3                      6  \n",
       "99                   4                      6  \n",
       "\n",
       "[100 rows x 25 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your DataFrame 'df' here\n",
    "\n",
    "# Specify the columns for imputation\n",
    "columns_to_impute = ['travelDurationNum', 'totalTravelDistance']  # Add more columns as needed\n",
    "\n",
    "# Replace missing values with zeros in the specified columns\n",
    "df[columns_to_impute] = df[columns_to_impute].fillna(0)\n",
    "\n",
    "# Print the DataFrame with replaced values\n",
    "df.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "ec8e2ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 29321.01\n",
      "R-squared (R2): 0.52\n",
      "Model Coefficients:\n",
      "startingAirportNum: 10.60\n",
      "destinationAirportNum: -0.66\n",
      "flightDateNum: 0.00\n",
      "departure_time_bin: -3.74\n",
      "departure_day_of_week: 11.63\n",
      "departure_month: -45.13\n",
      "segmentsCabinCodeNum: 97.21\n",
      "totalTravelDistance: 0.14\n",
      "travelDurationNum: 12.35\n",
      "Intercept: -58239.32\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# Define all features and target variable\n",
    "features = [\n",
    "    'startingAirportNum',\n",
    "    'destinationAirportNum',\n",
    "    'flightDateNum',  \n",
    "    'departure_time_bin',\n",
    "    'departure_day_of_week',\n",
    "    'departure_month',\n",
    "    'segmentsCabinCodeNum',\n",
    "    'totalTravelDistance',\n",
    "    'travelDurationNum'\n",
    "]\n",
    "\n",
    "target = 'totalFare'\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the model's performance metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R-squared (R2): {r2:.2f}\")\n",
    "\n",
    "# Print the model's coefficients and intercept\n",
    "print(\"Model Coefficients:\")\n",
    "for feature, coefficient in zip(features, model.coef_):\n",
    "    print(f\"{feature}: {coefficient:.2f}\")\n",
    "print(f\"Intercept: {model.intercept_:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "11463da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 4710.33\n",
      "R-squared (R2): 0.92\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define all features and target variable\n",
    "features = [\n",
    "    'startingAirportNum',\n",
    "    'destinationAirportNum',\n",
    "    'flightDateNum',  \n",
    "    'departure_time_bin',\n",
    "    'departure_day_of_week',\n",
    "    'departure_month',\n",
    "    'segmentsCabinCodeNum',\n",
    "    'totalTravelDistance',\n",
    "    'travelDurationNum'\n",
    "]\n",
    "\n",
    "target = 'totalFare'\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest regression model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)  # You can adjust the number of estimators as needed\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the model's performance metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R-squared (R2): {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d9f26923",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[215], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Use GridSearchCV to find the best hyperparameters\u001b[39;00m\n\u001b[0;32m     38\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(model, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Get the best hyperparameters\u001b[39;00m\n\u001b[0;32m     42\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:729\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    727\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 729\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    459\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    460\u001b[0m )(\n\u001b[0;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    462\u001b[0m         t,\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[0;32m    464\u001b[0m         X,\n\u001b[0;32m    465\u001b[0m         y,\n\u001b[0;32m    466\u001b[0m         sample_weight,\n\u001b[0;32m    467\u001b[0m         i,\n\u001b[0;32m    468\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[0;32m    469\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    470\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m    471\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \n\u001b[0;32m   1294\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1320\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m   1321\u001b[0m         X,\n\u001b[0;32m   1322\u001b[0m         y,\n\u001b[0;32m   1323\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1324\u001b[0m         check_input\u001b[38;5;241m=\u001b[39mcheck_input,\n\u001b[0;32m   1325\u001b[0m     )\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define all features and target variable\n",
    "features = [\n",
    "    'startingAirportNum',\n",
    "    'destinationAirportNum',\n",
    "    'flightDateNum',  \n",
    "    'departure_time_bin',\n",
    "    'departure_day_of_week',\n",
    "    'departure_month',\n",
    "    'segmentsCabinCodeNum',\n",
    "    'totalTravelDistance',\n",
    "    'travelDurationNum'\n",
    "]\n",
    "\n",
    "target = 'totalFare'\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a range of hyperparameters to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create a Random Forest regression model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create a Random Forest model with the best hyperparameters\n",
    "best_model = RandomForestRegressor(**best_params, random_state=42)\n",
    "\n",
    "# Fit the best model to the training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the best hyperparameters and model's performance metrics\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R-squared (R2): {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "5091264b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 20520.92\n",
      "R-squared (R2): 0.67\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define all features and target variable\n",
    "features = [\n",
    "    'startingAirportNum',\n",
    "    'destinationAirportNum',\n",
    "    'flightDateNum',  \n",
    "    'departure_time_bin',\n",
    "    'departure_day_of_week',\n",
    "    'departure_month',\n",
    "    'segmentsCabinCodeNum',\n",
    "    'totalTravelDistance',\n",
    "    'travelDurationNum'\n",
    "]\n",
    "\n",
    "target = 'totalFare'\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gradient Boosting regression model\n",
    "model = GradientBoostingRegressor(n_estimators=100, random_state=42)  # You can adjust the number of estimators as needed\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the model's performance metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R-squared (R2): {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b199a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define all features and target variable\n",
    "features = [\n",
    "    'startingAirportNum',\n",
    "    'destinationAirportNum',\n",
    "    'flightDateNum',  \n",
    "    'departure_time_bin',\n",
    "    'departure_day_of_week',\n",
    "    'departure_month',\n",
    "    'segmentsCabinCodeNum',\n",
    "    'totalTravelDistance',\n",
    "    'travelDurationNum'\n",
    "]\n",
    "\n",
    "target = 'totalFare'\n",
    "\n",
    "df_top_1000 = df.head(1000)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df_top_1000[features]\n",
    "y = df_top_1000[target]\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "#X = df[features]\n",
    "#y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Support Vector Regression (SVR) model\n",
    "model = SVR(kernel='linear', C=1.0, epsilon=0.1)  # You can adjust kernel, C, and epsilon as needed\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the model's performance metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R-squared (R2): {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "a9a26352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression Metrics:\n",
      "Mean Squared Error (MSE): 29321.01\n",
      "R-squared (R2): 0.52\n",
      "Lasso Regression Metrics:\n",
      "Mean Squared Error (MSE): 29362.41\n",
      "R-squared (R2): 0.52\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define all features and target variable\n",
    "features = [\n",
    "    'startingAirportNum',\n",
    "    'destinationAirportNum',\n",
    "    'flightDateNum',  \n",
    "    'departure_time_bin',\n",
    "    'departure_day_of_week',\n",
    "    'departure_month',\n",
    "    'segmentsCabinCodeNum',\n",
    "    'totalTravelDistance',\n",
    "    'travelDurationNum'\n",
    "]\n",
    "\n",
    "target = 'totalFare'\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Ridge regression model\n",
    "ridge_model = Ridge(alpha=1.0)  # You can adjust the alpha (regularization strength) as needed\n",
    "\n",
    "# Fit the Ridge model to the training data\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data using the Ridge model\n",
    "ridge_predictions = ridge_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Ridge model\n",
    "ridge_mse = mean_squared_error(y_test, ridge_predictions)\n",
    "ridge_r2 = r2_score(y_test, ridge_predictions)\n",
    "\n",
    "# Create a Lasso regression model\n",
    "lasso_model = Lasso(alpha=1.0)  # You can adjust the alpha (regularization strength) as needed\n",
    "\n",
    "# Fit the Lasso model to the training data\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data using the Lasso model\n",
    "lasso_predictions = lasso_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Lasso model\n",
    "lasso_mse = mean_squared_error(y_test, lasso_predictions)\n",
    "lasso_r2 = r2_score(y_test, lasso_predictions)\n",
    "\n",
    "# Print the model's performance metrics for both Ridge and Lasso\n",
    "print(\"Ridge Regression Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {ridge_mse:.2f}\")\n",
    "print(f\"R-squared (R2): {ridge_r2:.2f}\")\n",
    "\n",
    "print(\"Lasso Regression Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {lasso_mse:.2f}\")\n",
    "print(f\"R-squared (R2): {lasso_r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e476440a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 10027.69\n",
      "R-squared (R2): 0.84\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define all features and target variable\n",
    "features = [\n",
    "    'startingAirportNum',\n",
    "    'destinationAirportNum',\n",
    "    'flightDateNum',  \n",
    "    'departure_time_bin',\n",
    "    'departure_day_of_week',\n",
    "    'departure_month',\n",
    "    'segmentsCabinCodeNum',\n",
    "    'totalTravelDistance',\n",
    "    'travelDurationNum'\n",
    "]\n",
    "\n",
    "target = 'totalFare'\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a K-Nearest Neighbors (KNN) Regression model\n",
    "model = KNeighborsRegressor(n_neighbors=5)  # You can adjust the number of neighbors as needed\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the model's performance metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R-squared (R2): {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "fc7d4ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 5518.75\n",
      "R-squared (R2): 0.91\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define all features and target variable\n",
    "features = [\n",
    "    'startingAirportNum',\n",
    "    'destinationAirportNum',\n",
    "    'flightDateNum',  \n",
    "    'departure_time_bin',\n",
    "    'departure_day_of_week',\n",
    "    'departure_month',\n",
    "    'segmentsCabinCodeNum',\n",
    "    'totalTravelDistance',\n",
    "    'travelDurationNum'\n",
    "]\n",
    "\n",
    "target = 'totalFare'\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Decision Tree Regression model\n",
    "model = DecisionTreeRegressor(random_state=42)  # You can adjust hyperparameters as needed\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the model's performance metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R-squared (R2): {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0012bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define all features and target variable\n",
    "features = [\n",
    "    'startingAirportNum',\n",
    "    'destinationAirportNum',\n",
    "    'flightDateNum',  \n",
    "    'departure_time_bin',\n",
    "    'departure_day_of_week',\n",
    "    'departure_month',\n",
    "    'segmentsCabinCodeNum',\n",
    "    'totalTravelDistance',\n",
    "    'travelDurationNum'\n",
    "]\n",
    "\n",
    "target = 'totalFare'\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a range of hyperparameters to search\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create a Decision Tree regression model\n",
    "model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create a Decision Tree model with the best hyperparameters\n",
    "best_model = DecisionTreeRegressor(**best_params, random_state=42)\n",
    "\n",
    "# Fit the best model to the training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the best hyperparameters and model's performance metrics\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R-squared (R2): {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "042e83d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 642\n",
      "[LightGBM] [Info] Number of data points in the train set: 767799, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 450.661413\n",
      "Mean Squared Error (MSE): 14399.50\n",
      "R-squared (R2): 0.77\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "df['travelDurationNum'] = pd.to_numeric(df['travelDurationNum'], errors='coerce')\n",
    "\n",
    "# 'coerce' will convert non-numeric values to NaN\n",
    "\n",
    "# Check for missing or non-numeric values\n",
    "missing_values = df['travelDurationNum'].isnull().sum()\n",
    "if missing_values > 0:\n",
    "    print(f\"Found {missing_values} missing or non-numeric values in 'travelDurationNum' column. You may need to handle them.\")\n",
    "\n",
    "\n",
    "# Define all features and target variable\n",
    "features = [\n",
    "    'startingAirportNum',\n",
    "    'destinationAirportNum',\n",
    "    'flightDateNum',\n",
    "    'departure_time_bin',\n",
    "    'departure_day_of_week',\n",
    "    'departure_month',\n",
    "    'segmentsCabinCodeNum',\n",
    "    'totalTravelDistance',\n",
    "    'travelDurationNum'\n",
    "]\n",
    "\n",
    "target = 'totalFare'\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a LightGBM Regression model\n",
    "model = lgb.LGBMRegressor(random_state=42)  # You can adjust hyperparameters as needed\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the model's performance metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R-squared (R2): {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd2b38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
